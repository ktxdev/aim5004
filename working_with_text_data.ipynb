{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d92d39e-4cdc-449b-946c-a8aead2324de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit, cross_val_score \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c7d736f-4093-4532-8566-91957eebbf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "length of text_train: 25000\n",
      "text_train[1]:\n",
      "b'Words can\\'t describe how bad this movie is. I can\\'t explain it by writing only. You have too see it for yourself to get at grip of how horrible a movie really can be. Not that I recommend you to do that. There are so many clich\\xc3\\xa9s, mistakes (and all other negative things you can imagine) here that will just make you cry. To start with the technical first, there are a LOT of mistakes regarding the airplane. I won\\'t list them here, but just mention the coloring of the plane. They didn\\'t even manage to show an airliner in the colors of a fictional airline, but instead used a 747 painted in the original Boeing livery. Very bad. The plot is stupid and has been done many times before, only much, much better. There are so many ridiculous moments here that i lost count of it really early. Also, I was on the bad guys\\' side all the time in the movie, because the good guys were so stupid. \"Executive Decision\" should without a doubt be you\\'re choice over this one, even the \"Turbulence\"-movies are better. In fact, every other movie in the world is better than this one.'\n"
     ]
    }
   ],
   "source": [
    "reviews_train = load_files(\"data/aclImdb/train/\")\n",
    "\n",
    "# load_files returns a bunch, containing training texts and training labels\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "\n",
    "print(\"type of text_train: {}\".format(type(text_train)))\n",
    "print(\"length of text_train: {}\".format(len(text_train)))\n",
    "print(\"text_train[1]:\\n{}\".format(text_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a19b5420-1a49-4d10-bf01-f9199c0f88a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3141821-4265-4d38-be22-6aa7c9ab420a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class (training): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "print(\"Samples per class (training): {}\".format(np.bincount(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee8cc591-3b53-4006-9320-bf5fd6fb3a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in test data: 25000\n",
      "Samples per class (test): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "reviews_test = load_files(\"data/aclImdb/test/\")\n",
    "\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "print(\"Number of documents in test data: {}\".format(len(text_test)))\n",
    "print(\"Samples per class (test): {}\".format(np.bincount(y_test)))\n",
    "\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73f98345-b3aa-4999-962b-6b27e6b3a15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 3431196 stored elements and shape (25000, 74849)>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer().fit(text_train)\n",
    "\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d69dc3b1-65e6-43fa-bf5f-f1e36be0be53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 74849\n",
      "First 20 features:\n",
      "['00' '000' '0000000000001' '00001' '00015' '000s' '001' '003830' '006'\n",
      " '007' '0079' '0080' '0083' '0093638' '00am' '00pm' '00s' '01' '01pm' '02']\n",
      "Features 20010 to 20030:\n",
      "['dratted' 'draub' 'draught' 'draughts' 'draughtswoman' 'draw' 'drawback'\n",
      " 'drawbacks' 'drawer' 'drawers' 'drawing' 'drawings' 'drawl' 'drawled'\n",
      " 'drawling' 'drawn' 'draws' 'draza' 'dre' 'drea']\n",
      "Every 2000th feature:\n",
      "['00' 'aesir' 'aquarian' 'barking' 'blustering' 'bÃªte' 'chicanery'\n",
      " 'condensing' 'cunning' 'detox' 'draper' 'enshrined' 'favorit' 'freezer'\n",
      " 'goldman' 'hasan' 'huitieme' 'intelligible' 'kantrowitz' 'lawful' 'maars'\n",
      " 'megalunged' 'mostey' 'norrland' 'padilla' 'pincher' 'promisingly'\n",
      " 'receptionist' 'rivals' 'schnaas' 'shunning' 'sparse' 'subset'\n",
      " 'temptations' 'treatises' 'unproven' 'walkman' 'xylophonist']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names_out()\n",
    "\n",
    "print(\"Number of features: {}\".format(len(feature_names)))\n",
    "print(\"First 20 features:\\n{}\".format(feature_names[:20]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da81ee9a-5ffc-4d4d-9ab8-bfc8368e1fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "# Evaluating a logistic regression model\n",
    "scores = cross_val_score(LogisticRegression(max_iter=1000), X_train, y_train, cv=5)\n",
    "print(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23e00e63-b4fd-44a4-a2cd-f96d43da64b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.89\n",
      "Best parameters:  {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9e5f69a-f13d-4c72-b141-b2f0bf6ed66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88\n"
     ]
    }
   ],
   "source": [
    "# Assesing generalization performance\n",
    "X_test = vect.transform(text_test)\n",
    "\n",
    "print(\"{:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20705bff-5c12-4e33-a9f2-5c9cf30929d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with min_df: <Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 3354014 stored elements and shape (25000, 27271)>\n"
     ]
    }
   ],
   "source": [
    "# Limiting tokens to only those that appear in at least 5 documents\n",
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "print(\"X_train with min_df: {}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f748d7c5-2a0c-4c2e-9e73-a179ce3710ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 features:\n",
      "['00' '000' '007' '00s' '01' '02' '03' '04' '05' '06' '07' '08' '09' '10'\n",
      " '100' '1000' '100th' '101' '102' '103' '104' '105' '107' '108' '10s'\n",
      " '10th' '11' '110' '112' '116' '117' '11th' '12' '120' '12th' '13' '135'\n",
      " '13th' '14' '140' '14th' '15' '150' '15th' '16' '160' '1600' '16mm' '16s'\n",
      " '16th']\n",
      "Features 20010 to 20030:\n",
      "['repentance' 'repercussions' 'repertoire' 'repetition' 'repetitions'\n",
      " 'repetitious' 'repetitive' 'rephrase' 'replace' 'replaced' 'replacement'\n",
      " 'replaces' 'replacing' 'replay' 'replayable' 'replayed' 'replaying'\n",
      " 'replays' 'replete' 'replica']\n",
      "Every 700th feature:\n",
      "['00' 'affections' 'appropriately' 'barbra' 'blurbs' 'butchered' 'cheese'\n",
      " 'commitment' 'courts' 'deconstructed' 'disgraceful' 'dvds' 'eschews'\n",
      " 'fell' 'freezer' 'goriest' 'hauser' 'hungary' 'insinuate' 'juggle'\n",
      " 'leering' 'maelstrom' 'messiah' 'music' 'occasional' 'parking'\n",
      " 'pleasantville' 'pronunciation' 'recipient' 'reviews' 'sas' 'shea'\n",
      " 'sneers' 'steiger' 'swastika' 'thrusting' 'tvs' 'vampyre' 'westerns']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names_out()\n",
    "\n",
    "print(\"First 50 features:\\n{}\".format(feature_names[:50]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 700th feature:\\n{}\".format(feature_names[::700]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2214ee5-cb7c-42df-bd36-6250aa2c7987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.89\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d2d9e-85b8-4211-9782-2b4fe5c78f97",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e86b181c-dcaa-4a27-abe6-fa1163eeaa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 318\n",
      "Every 10th stopword:\n",
      "['own', 'for', 'has', 'alone', 'besides', 'who', 'out', 'both', 'him', 'becoming', 'became', 'which', 'done', 'can', 'but', 'them', 'detail', 'i', 'made', 'then', 'see', 'one', 'hereupon', 'anyone', 'since', 'sixty', 'put', 'why', 'whatever', 'beforehand', 'well', 'elsewhere']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\n",
    "print(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44e69fe8-f59b-4278-82c9-aab046f8b537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with stop words:\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 2149958 stored elements and shape (25000, 26966)>\n"
     ]
    }
   ],
   "source": [
    "# Specifying stop_words=\"english\" uses the built-in list.\n",
    "# We could also augment it and pass our own.\n",
    "vect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\n",
    "\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "print(\"X_train with stop words:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a197a40d-eb7a-42b1-9283-84b2e4a84298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.88\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e44426d0-ec0c-4f45-a41c-25c09b937982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.89\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None), LogisticRegression(max_iter=1000))\n",
    "\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4771555-d6bd-40fe-b5dd-9db962f440b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest tfidf:\n",
      "['poignant' 'disagree' 'instantly' 'importantly' 'lacked' 'occurred'\n",
      " 'currently' 'altogether' 'nearby' 'undoubtedly' 'directs' 'fond'\n",
      " 'stinker' 'avoided' 'emphasis' 'commented' 'disappoint' 'realizing'\n",
      " 'downhill' 'inane']\n",
      "Features with highest tfidf: \n",
      "['coop' 'homer' 'dillinger' 'hackenstein' 'gadget' 'taker' 'macarthur'\n",
      " 'vargas' 'jesse' 'basket' 'dominick' 'the' 'victor' 'bridget' 'victoria'\n",
      " 'khouri' 'zizek' 'rob' 'timon' 'titanic']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "\n",
    "# transform the training dataset\n",
    "X_train = vectorizer.transform(text_train)\n",
    "\n",
    "# find maximum value for each of the features over the dataset\n",
    "max_value = X_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "\n",
    "# get feature names\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(feature_names[sorted_by_tfidf[:20]]))\n",
    "print(\"Features with highest tfidf: \\n{}\".format(feature_names[sorted_by_tfidf[-20:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df50c66c-2ea1-418f-beb2-83e994d252b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest idf:\n",
      "['the' 'and' 'of' 'to' 'this' 'is' 'it' 'in' 'that' 'but' 'for' 'with'\n",
      " 'was' 'as' 'on' 'movie' 'not' 'have' 'one' 'be' 'film' 'are' 'you' 'all'\n",
      " 'at' 'an' 'by' 'so' 'from' 'like' 'who' 'they' 'there' 'if' 'his' 'out'\n",
      " 'just' 'about' 'he' 'or' 'has' 'what' 'some' 'good' 'can' 'more' 'when'\n",
      " 'time' 'up' 'very' 'even' 'only' 'no' 'would' 'my' 'see' 'really' 'story'\n",
      " 'which' 'well' 'had' 'me' 'than' 'much' 'their' 'get' 'were' 'other'\n",
      " 'been' 'do' 'most' 'don' 'her' 'also' 'into' 'first' 'made' 'how' 'great'\n",
      " 'because' 'will' 'people' 'make' 'way' 'could' 'we' 'bad' 'after' 'any'\n",
      " 'too' 'then' 'them' 'she' 'watch' 'think' 'acting' 'movies' 'seen' 'its'\n",
      " 'him']\n"
     ]
    }
   ],
   "source": [
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "\n",
    "print(\"Features with lowest idf:\\n{}\".format(feature_names[sorted_by_idf[:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82ab1219-c0c3-4893-96d6-cb55a3a49f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.91\n",
      "Best parameters:\n",
      "{'logisticregression__C': 100, 'tfidfvectorizer__ngram_range': (1, 3)}\n"
     ]
    }
   ],
   "source": [
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression(max_iter=1000))\n",
    "\n",
    "# running the grid search takes a long time because of the\n",
    "# relatively large grid and the inclusion of trigrams\n",
    "param_grid = {\n",
    "    \"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters:\\n{}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6182a57-f0c4-4caf-81a2-60cb4b3c0da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_lemma.shape: (25000, 21577)\n",
      "X_train.shape: (25000, 27271)\n"
     ]
    }
   ],
   "source": [
    "# Technicality: we want to use the regexp-based tokenizer\n",
    "# that is used by CountVectorizer and only use the lemmatization\n",
    "# from spacy. To this end, we replace en_nlp.tokenizer (the spacy tokenizer)\n",
    "# with the regexp-based tokenization.\n",
    "\n",
    "# regexp used in CountVectorizer\n",
    "regexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "\n",
    "# load spacy language model and save old tokenizer\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "old_tokenizer = en_nlp.tokenizer\n",
    "\n",
    "# replace the tokenizer with the preceding regexp\n",
    "en_nlp.tokenizer = lambda string: Doc(en_nlp.vocab, words=regexp.findall(string))\n",
    "\n",
    "# create a custom tokenizer using the spacy document processing pipeline\n",
    "# (now using our own tokenizer)\n",
    "def custom_tokenizer(document):\n",
    "    doc_spacy = en_nlp(document, disable=[\"ner\", \"parser\"])\n",
    "    return [token.lemma_ for token in doc_spacy]\n",
    "\n",
    "# define a count vectorizer with the custom tokenizer\n",
    "lemma_vect = CountVectorizer(tokenizer=custom_tokenizer, min_df=5)\n",
    "\n",
    "# transform text_train using CountVectorizer with lemmatization\n",
    "X_train_lemma = lemma_vect.fit_transform(text_train)\n",
    "\n",
    "print(\"X_train_lemma.shape: {}\".format(X_train_lemma.shape))\n",
    "\n",
    "# standard CountVectorizer for reference\n",
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4ddaefab-fd65-4cf0-99bb-2615a1740ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score (standard CountVectorizer): 0.719\n",
      "Best cross-validation score (lemmatization): 0.728\n"
     ]
    }
   ],
   "source": [
    "# build a grid search using only 1% of the data as the training set\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.99, train_size=0.01, random_state=0)\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=cv)\n",
    "\n",
    "# perform grid search with standard CountVectorizer\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score (standard CountVectorizer): {:.3f}\".format(grid.best_score_))\n",
    "\n",
    "# perform grid search with lemmatization\n",
    "grid.fit(X_train_lemma, y_train)\n",
    "\n",
    "print(\"Best cross-validation score (lemmatization): {:.3f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6e89f49d-2f4b-49e4-bf5c-393f08690a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Topic modelling\n",
    "vect = CountVectorizer(max_features=10000, max_df=.15)\n",
    "X = vect.fit_transform(text_train)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, learning_method=\"batch\", max_iter=25, random_state=0)\n",
    "\n",
    "# We build the model and transform the data in one step\n",
    "# Computing transform takes some time,\n",
    "# and we can save time by doing both at once\n",
    "document_topics = lda.fit_transform(X)\n",
    "\n",
    "print(lda.components_.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
