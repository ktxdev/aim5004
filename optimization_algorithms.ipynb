{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZ/QMKQJix2VJgFr+sbkPe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ktxdev/ml_ai/blob/main/optimization_algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Optimization algorithms are mathematical techniques used to identify the best solution by minimizing or maximizing an objective function, playing a crucial role in fields such as machine learning and operations research. This exploration focuses on three prominent algorithms: Gradient Descent, which iteratively adjusts solutions in the direction of the steepest descent to find local or global minima; Simulated Annealing, a probabilistic approach inspired by metallurgy that escapes local optima by accepting worse solutions with decreasing probability, making it effective for discrete and combinatorial problems; and Evolutionary Computing (Genetic Algorithms), which evolves a population of solutions through natural selection mechanisms like selection, crossover, and mutation, suited for complex and non-differentiable challenges. Each algorithm has unique strengths and is designed for different optimization problems, and the following sections will detail their working principles, strengths and weaknesses, along with Python implementations.\n",
        "\n",
        "# Gradient Descent\n",
        "- Gradient Descent is an iterative optimization algorithm used to minimize a function by moving in the direction of the steepest descent, i.e., the negative of the gradient.\n",
        "- For a given function $f(x)$, the gradient is the vector of its partial derivatives.\n",
        "- Gradient Descent takes small steps proportional to the negative of the gradient until it converges to a local or global minimum.\n",
        "- **Types of Gradient Descent:**\n",
        "  1. **Batch Gradient Descent:** Computes the gradient using the whole dataset in each iteration.\n",
        "  2. **Stochastic Gradient Descent (SGD):** Uses one random sample to compute the gradient.\n",
        "  3. **Mini-batch Gradient Descent:** Uses a small subset (mini-batch) of data to compute the gradient.\n",
        "- **Use cases include:** linaer/logistic regression, neural networks training and convex optimization problems where global minimum exists\n",
        "- It is best suited for convex optimization problems or problems where derivatives are easy to compute.\n"
      ],
      "metadata": {
        "id": "ElfNQjcIJu89"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUPXH0AaJqSL",
        "outputId": "a93358cd-22f5-4d69-c6b4-c783bb9dad61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum point: 0.00014272476927059603\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent(gradient: callable, start: float, learn_rate: float, n_iter: int = 50, tolerance: float = 1e-06) -> float:\n",
        "  \"\"\"Perform gradient descent\n",
        "\n",
        "  Parameters:\n",
        "    gradient (callable): A function that returns the gradient of the objective function\n",
        "    start (float): The initial starting point for the algorithm\n",
        "    learn_rate (float): A step size that determines how far we move along the gradient\n",
        "    n_iter (int): Maximum number of iterations (default: 50)\n",
        "    tolerance (float): A number that determines the convergence threshold\n",
        "\n",
        "  Returns:\n",
        "    float: The minimum found by the algorithm\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize x to the starting point\n",
        "  x = start\n",
        "\n",
        "  # Loop over the number of iterations\n",
        "  for _ in range(n_iter):\n",
        "    # Compute change in x\n",
        "    diff = -learn_rate * gradient(x)\n",
        "\n",
        "    # If converged break the loop\n",
        "    if np.all(np.abs(diff) <= tolerance):\n",
        "      break\n",
        "\n",
        "    # Update current value of x\n",
        "    x += diff\n",
        "\n",
        "  return x\n",
        "\n",
        "def gradient(x):\n",
        "  \"\"\"Defines the gradient of the function f(x) = x^2\n",
        "  \"\"\"\n",
        "  return 2 * x\n",
        "\n",
        "# Run gradient descent starting at x = 10 with learning rate of 0.1\n",
        "result = gradient_descent(gradient, start = 10, learn_rate = 0.1)\n",
        "print(f\"Minimum point: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulated Annealing (SA)\n",
        "- The algorithm begins with an initial solution and iteratively makes small changes.\n",
        "- At each iteration, the algorithm probabilistically decides whether to accept worse solutions to avoid getting stuck in local minima.\n",
        "- As iterations progress, the acceptance of worse solutions decreases.\n",
        "- The key to SA is the temperature parameter, which starts high and gradually decreases.\n",
        "- Early in the process, the algorithm can accept worse solutions to explore the solution space.\n",
        "- As the temperature decreases, the algorithm becomes more conservative and focuses on refining the current solution.\n",
        "- **Use cases include:** Comninational optimization problems, Traveling Salesman Problem and Scheduling problems\n",
        "- It is best suited for non-convex, discrete, or combinatorial optimization problems where there is a risk of getting stuck in local minima"
      ],
      "metadata": {
        "id": "8YHttV5IStmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simulated_annealing(func: callable, bounds: tuple, temp: float = 1000, cooling_rate: float = 0.95, max_iter: int = 1000) -> tuple:\n",
        "  \"\"\"Performs Simulated Annealing algorithm\n",
        "\n",
        "    Parameters:\n",
        "      func (callable): The objective function to be minimized\n",
        "      bounds (tuple): The limits for the search space\n",
        "      temp (float): The initial temperature (default = 1000)\n",
        "      cooling_rate (float): The rate at which the temperature is decreased (default = 0.95)\n",
        "      max_iter (int): Maximum number of iterations (default = 1000)\n",
        "\n",
        "    Returns:\n",
        "      tuple: The best solution found and it's evaluation\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize current solution, and best solution\n",
        "  best = current = np.random.uniform(bounds[0], bounds[1])\n",
        "  # Set current evalution and best evaluation\n",
        "  best_eval = current_eval = func(current)\n",
        "\n",
        "  # Loop over the number of maximum iterations\n",
        "  for i in range(max_iter):\n",
        "    # Generate a new candidate solution by adding a number between -1 and 1 to current solution\n",
        "    candidate = current + np.random.uniform(-1, 1)\n",
        "    candidate_eval = func(candidate)\n",
        "\n",
        "    # Evaluate the new candidate with the objective function\n",
        "    if candidate_eval < current_eval or np.random.rand() < np.exp((current_eval - candidate_eval) / temp):\n",
        "      current, current_eval = candidate, candidate_eval\n",
        "\n",
        "    # Update best solution if current solution is better than the one found so far\n",
        "    if current_eval < best_eval:\n",
        "      best, best_eval = current, current_eval\n",
        "\n",
        "    # Decrease the temprature by the cooling rate\n",
        "    temp *= cooling_rate\n",
        "\n",
        "  return best, best_eval\n",
        "\n",
        "def objective(x):\n",
        "  \"\"\"Defines the objective function f(x) = x^2\n",
        "  \"\"\"\n",
        "  return x ** 2\n",
        "\n",
        "# Run Simulated annealing on the function f(x) = x^2 with limits between -10 and 10\n",
        "best, best_eval = simulated_annealing(objective, bounds=(-10, 10))\n",
        "print(f\"Best solution: {best}, Objective value: {best_eval}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrRtogHmJqZK",
        "outputId": "86a5d64b-db07-40a0-a903-2e8c3aa5d2fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best solution: 0.0036999266794541796, Objective value: 1.3689457433336832e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evolutionary Computing (Genetic Algorithm)\n",
        "- Evolutionary Computing (EC) is inspired by the process of natural selection.\n",
        "- This class of algorithms, including Genetic Algorithms (GA), solves optimization problems by evolving a population of candidate solutions over time.\n",
        "- The process involves:\n",
        "  - **Selection:** Choosing individuals based on fitness.\n",
        "  - **Crossover (Recombination):** Combining parts of two individuals to produce offspring.\n",
        "  - **Mutation:** Introducing random changes to individuals to maintain diversity.\n",
        "- Each iteration, known as a generation, involves evaluating the fitness of individuals and selecting the best to form the next generation.\n",
        "- **Use cases include:** Optimization problems with multiple objectives, Feature selection in machine learning and Game playing strategies\n",
        "- It is best suited for problems where the solution space is large, complex, and non-differentiable, and for problems that require global optimization."
      ],
      "metadata": {
        "id": "tJEU9rV-Wg3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def genetic_algorithm(func: callable, bounds: tuple, pop_size: int = 100, n_generations: int = 100, mutation_rate: float = 0.1):\n",
        "  \"\"\"Performs the genetic algorithm\n",
        "\n",
        "  Parameters:\n",
        "    func (callable): The objective function to minimize\n",
        "    bounds (tuple): The bounds for the search space\n",
        "    pop_size (int): size of the population\n",
        "    n_generation (int): number of generations for evolution (default = 100)\n",
        "    mutation_rate (float): Probability of mutation (default = 0.1)\n",
        "\n",
        "  Returns:\n",
        "  \"\"\"\n",
        "  # Initialize population with random values between the bounds\n",
        "  population = np.random.uniform(bounds[0], bounds[1], (pop_size, 1))\n",
        "\n",
        "  # Loop over number of generations\n",
        "  for generation in range(n_generations):\n",
        "    # Evaluate the fitness of each individual by applying the objective function\n",
        "    # to evry individual in the population\n",
        "    fitness = np.array([func(ind) for ind in population]).flatten()\n",
        "    # Select first half of the population as parents for the next generation\n",
        "    sorted_indices = np.argsort(fitness)  # Indices of sorted fitness\n",
        "    parents = population[sorted_indices][:pop_size // 2]  # Shape: (pop_size // 2, 1)\n",
        "\n",
        "    offspring = []\n",
        "    for _ in range(pop_size//2):\n",
        "      # Randomly select two parents for each new individual to be created\n",
        "      parent1, parent2 = parents[np.random.randint(0, len(parents), 2)]\n",
        "      # Create new child by averaging the values of the two parents\n",
        "      child = (parent1 + parent2) / 2\n",
        "      if np.random.rand() < mutation_rate:\n",
        "          child += np.random.uniform(-1, 1)\n",
        "      offspring.append(child)\n",
        "\n",
        "    # Form new population by combining the parents and the offspring\n",
        "    population = np.vstack((parents, offspring))\n",
        "\n",
        "  best_individual = population[np.argmin([func(ind) for ind in population])]\n",
        "  return best_individual\n",
        "\n",
        "def fitness(x):\n",
        "  \"\"\"Defines the fittness function f(x) = x^2\n",
        "  \"\"\"\n",
        "  return x ** 2\n",
        "\n",
        "best = genetic_algorithm(fitness, bounds=(-10, 10))\n",
        "print(f\"Best individual: {best}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q65Ov75eWgSQ",
        "outputId": "15fea3a0-112a-4870-f016-dd45bf8357d5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best individual: [4.60606805e-42]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Strengths and Weaknesses\n",
        "|Algorithm|Strengths|Weaknesses|\n",
        "|---------|---------|----------|\n",
        "|Gradient Decent|- Fast convergence on convex problems|- Prone to getting stuck in local minima|\n",
        "| |- Simple to implement and understand|- Requires differentiability|\n",
        "| |- Efficient for large-scale linear models|- Learning rate tuning can be difficult|\n",
        "|Simulated Annealing|- Can escape local minima|- Slow convergence|\n",
        "| |- Simple and flexible|- Requires careful temperature schedule design|\n",
        "| |- Can handle noisy and discontinuous functions|- No guarantee of finding the global optimum|\n",
        "|Evolutionary Computing|- Robust to non-differentiable, noisy, or complex landscapes|- Computationally expensive|\n",
        "| |- Can handle multiple objectives|- Requires tuning multiple hyperparameters (mutation rate, etc.)|\n",
        "| |- Global optimization potential|- Can converge prematurely if diversity is lost|"
      ],
      "metadata": {
        "id": "AKpffJUibNGv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vlHWnVz7ToUP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}